url = "https://finance.yahoo.com/industry/agricultural_chemicals"
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec[56:length(sec)]
sec[1]
sec[56]
sec[57]
sec[58]
sec[59]
sec[60]
sec[61]
sec[62]
sec[63]
sec[78]
sec[79]
sec <- sec[56:length(sec)]
html <- htmlParse(sec)
html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec
html <- htmlParse(sec)
html
html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names
yahoo.download.as.df <- function(url, set.id=TRUE) {
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec <- sec[56:length(sec)]
html <- htmlParse(sec)
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td/', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
html.names <- substr(html.names, 81, nchar(html.names)-5)
}
html.names <- as.vector(xpathSApply(html, '//td/', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names
html.names <- html.names[!is.na(html.names)]
html.names <- html.names[!is.na(html.names)]
html.names
html.names <- substr(html.names, 81, nchar(html.names)-5)
html.names
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
html.names
nchar(html.names)
html.names <- substr(html.names, (nchar(html.names)+10)/2, nchar(html.names))
html.names
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
html.names
yahoo.download.as.df <- function(url, set.id=TRUE) {
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec <- sec[56:length(sec)]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
}
stock <- industry.All.companies(112)
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec
html
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec <- sec[56:length(sec)]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec <- sec[1:length(sec)]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec <- sec[1:56]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
html.names
sec[1:56]
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[1:56]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
html.names
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[30:56]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
}
html.names
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[10:30]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
html.names
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[1:30]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
html.names
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[1:20]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
html.names
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
sec <- sec[1:20]
sec
sec <- sec[10:20]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[1:10]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
html.names
sec
yahoo.download.as.df <- function(url, set.id=TRUE) {
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[1:10]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
}
industry.All.companies(30)
industry.All.companies(112)
industry.All.companies(111)
industry.All.companies(1)
stock <- industry.All.companies(112)
stock
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[1:10]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
yahoo.download.as.df(url)
df <- yahoo.download.as.df(url)
df
url
industry.All.companies(111)
df <- df[df != ""]
df
url <- paste(url.yahoo.finance.base, paste(as.integer(111), 'conameu.html', sep=''), sep='/')
url
url <- paste(url.yahoo.finance.base,
paste(as.integer(industry), '/conameu.html', sep=''),
url <- paste(url.yahoo.finance.base,
paste(as.integer(industry), '/conameu.html', sep=''),
sep='/')
industry <- 111
url <- paste(url.yahoo.finance.base,
paste(as.integer(industry), '/conameu.html', sep=''),
sep='/')
url
df <- yahoo.download.as.df(url)
url.yahoo.finance.base <- 'https://biz.yahoo.com/p'
url <- paste(url.yahoo.finance.base,
paste(as.integer(industry), '/conameu.html', sep=''),
sep='/')
url
df <- yahoo.download.as.df(url)
df
url
listAll
industry.All.companies(112)
industry.All.companies(914)
# ----------------------------------------------------------
# Program to download financial info from google and yahoo
# and save it to disk to be accessed later
# It uses SymbolBySector.R
# ----------------------------------------------------------
# Load relevant packages
library(tseries)
library(quantmod)
targetPath <- "~/Dropbox/Courses/R/StockModel-I/Downloads_2017Aug/"
# Loading additional functions
source('~/Dropbox/Courses/R/StockModel-I/SymbolBySector.R')
fileName <- paste("~/Dropbox/Courses/R/StockModel-I/", "SectorIndustryInfo.RData", sep="")
load(fileName)  # loads listAll: all sector and industries
# Creating a table with the stock info  --------------------
stockInfo <- data.frame(Stock.SYM = character(0),
Sector.Num = numeric(0),
Industry.Num = numeric(0), stringsAsFactors=FALSE
)
# ---------------------------------------------------------------------------
# Creating data frame with all stock symbols, sector and industry numbers
# ---------------------------------------------------------------------------
stockInfoAll <- stockInfo
for (j in 1:length(listAll[,1])) {
# Selecting stocks of this sector-industry
stock <- industry.All.companies(listAll[j,4])
if (length(stock) > 0) {
for (i in 1:length(stock)) {
stockInfoAll[nrow(stockInfoAll) + 1, ] <- c(stock[i], listAll[j,2], listAll[j,4])
}
}
}
stockInfoAll
dim(stockInfoAll)
stockInfoAll[3000,]
yahoo.download.as.df <- function(url, set.id=TRUE) {
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
# sec <- sec[1:10]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
}
# --------
# Return a dataframe of companies in the specified sector. Note that sector is a numeric ID
# as provided by the dataframe returned by list.sectors.industries().
# sector.All.companies(sector num)
# --------
# Return a dataframe of companies in the specified industry. Note that industry is a numeric ID
# as provided by the  dataframe returned by list.sectors.industries().
# industry.All.companies(industry num)
# Creating a table with the stock info  --------------------
stockInfo <- data.frame(Stock.SYM = character(0),
Sector.Num = numeric(0),
Industry.Num = numeric(0), stringsAsFactors=FALSE
)
# ---------------------------------------------------------------------------
# Creating data frame with all stock symbols, sector and industry numbers
# ---------------------------------------------------------------------------
stockInfoAll <- stockInfo
for (j in 1:length(listAll[,1])) {
# Selecting stocks of this sector-industry
stock <- industry.All.companies(listAll[j,4])
if (length(stock) > 0) {
for (i in 1:length(stock)) {
stockInfoAll[nrow(stockInfoAll) + 1, ] <- c(stock[i], listAll[j,2], listAll[j,4])
}
}
}
dim(stockInfoAll)
yahoo.download.as.df <- function(url, set.id=TRUE) {
sec <- scan(file = url, what = "character", sep ="\n",  allowEscapes = TRUE)
# sec <- sec[1:length(sec)]
sec <- sec[1:10]
html <- htmlParse(sec)
#Yahoo finance changed some of its pages. Updated August 2017
#html.names <- as.vector(xpathSApply(html, '//td/font', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- as.vector(xpathSApply(html, '//td', function(x) ifelse(is.null(xmlChildren(x)$a), NA, xmlAttrs(xmlChildren(x)$a, 'href'))))
html.names <- html.names[!is.na(html.names)]
#Yahoo finance changed some of its pages. Updated August 2017
html.names <- substr(html.names, (nchar(html.names)+10)/2+1, nchar(html.names))
}
listAll
industry.All.companies(832)
industry.All.companies(834)
url <- paste(url.yahoo.finance.base,
paste(as.integer(834), '/conameu.html', sep=''),
sep='/')
url
stockInfoAll[1:10,]
stockInfoAll[1:20,]
stockInfoAll_new <- stockInfoAll
load("~/Dropbox/Courses/R/StockModel-I/Downloads/StockInfoAll.RData")
stockInfoAll[1:20,]
stockSymbols()
ttrSym <- stockSymbols()
dim(ttrSym)
ttrSym[1,]
ttrSym[2,]
ttrSym[3,]
ttrSym[4,]
ttrSym[5,]
ttrSym[6,]
ttrSym[7,]
ttrSym[8,]
ttrSym[9,]
ttrSym[10,]
ttrSym[11,]
ttrSym[12,]
# ----------------------------------------------------------
# StockModel.R is the main file
# It uses PrepareTable.R to construct tables with stock information
# It uses PrepareStockModel.R to construct stock model (and indirectly StockInfo.R)
# It uses saved info that has been previously created by running Download.R
# ----------------------------------------------------------
# Load relevant packages
library(quantmod)
library(PerformanceAnalytics)
library(zoo)
library(tseries)
library(ggplot2)
library(caret)
library(forecast)  # Holt-Winters and Arima forecasting method
# Parallel computing
library(doParallel)
registerDoParallel(cores=4)
# Loading info to be used ----------------------------------------
# Load data frame with Sector.Name, Sector.Num, Industry.Name, Industry.Num into listAll
load(file = "~/Dropbox/Courses/R/StockModel-I/SectorIndustryInfo.RData")
# Load stock, sector and industry information into StockInfo
load(file = "~/Dropbox/Courses/R/StockModel-I/Downloads_2017Aug/StockInfo.RData")
# Sourcing functions ----------------------------------------------
source('~/Dropbox/Courses/R/StockModel-I/PrepareTable.R')
# Sourcing prepare.table.sector function
source('~/Dropbox/Courses/R/StockModel-I/PrepareTableSector.R')
# Sourcing add.histo.to.table function
source('~/Dropbox/Courses/R/StockModel-I/StockInfoHistorical.R')
# Sourcing prepare.model function
source('~/Dropbox/Courses/R/StockModel-I/PrepareStockModel.R')
# Creating table to be used to create model -----------------------
# The model will be trained up to end.date.model. This date is currently set by earliest financial
# quaterly data from google financial
ini.date.model <- as.Date("2014/03/03")   # Initial date the model is prepared
histo.date.model <- as.Date("2016/03/31")   # date the model is compared to historical info (year earlier than end.date.model?)
end.date.model <- as.Date("2017/03/31")   # Final date the model is prepared -- should be end of month
apply.date.model <- as.Date("2017/06/30") # Date the model is designed to predict win/loss performance -- should be end of month
# Prepare table with stock info
table.model <- prepare.table(stockInfo, end.date.model, ini.date.model, apply.date.model)
table.model[1,]
# Removing stocks that may have problems
table.model <- table.model[table.model$Price.Model.end > 0.01 & table.model$Price.Min > 0.01,]
# Adding to table valuations compared to peers
table.model <- prepare.table.sector(table.model)
table.model[1,]
# Adding historical financial status comparison
table.model <- add.histo.to.table(table.model, histo.date.model)
table.model[1,]
save(table.model, file = "~/Dropbox/Courses/R/StockModel-I/Figures/Table-3m_2017-06-30.Rda")
# Dividing table into training and test data  ---------------------
set.seed(235)
inTrain <- createDataPartition(table.model$actual.win.loss, list = FALSE, p = 0.7)
my_train <- table.model[inTrain,]
my_val <- table.model[-inTrain,]
# Creating stock model with multiple methods ----------------------
model_ranger <- prepare.model(my_train, "ranger")    # Model ranger
my_val$ranger_pred <- predict(model_ranger, my_val)
model_gbm <- prepare.model(my_train, "gbm")          # Model gbm
my_val$gbm_pred <- predict(model_gbm, my_val)
model_glmnet <- prepare.model(my_train, "glmnet")    # Model glmnet
my_val$glmnet_pred <- predict(model_glmnet, my_val)
# Creating table with rankings from the different methods
ordered_actual <-      my_val[order(my_val$actual.win.loss),]
ordered_ranger <-      my_val[order(my_val$ranger_pred),]  #Ranger
ordered_gbm    <-      my_val[order(my_val$gbm_pred),]     #GBM
ordered_glmnet <-      my_val[order(my_val$glmnet_pred),]  #GLMNET
rank.robust <- data.frame(Stock.SYM = character(0), rank_ranger = numeric(0), rank_gbm = numeric(0), rank_glmnet = numeric(0),
rank_actual = numeric(0), actual.win.loss = numeric(0), stringsAsFactors=FALSE)
for (i in 1:length(ordered_ranger$Stock.SYM)) {
rank.robust[i,] <- list(ordered_ranger$Stock.SYM[i],                                                                         #Name stock
100.*i/length(ordered_ranger$Stock.SYM),                                                             #Rank ranger
100.*match(ordered_ranger$Stock.SYM[i], ordered_gbm$Stock.SYM)/length(ordered_ranger$Stock.SYM),     #Rank gbm
100.*match(ordered_ranger$Stock.SYM[i], ordered_glmnet$Stock.SYM)/length(ordered_ranger$Stock.SYM),  #Rank glmnet
100.*match(ordered_ranger$Stock.SYM[i], ordered_actual$Stock.SYM)/length(ordered_ranger$Stock.SYM),  #Rank actual
ordered_ranger$actual.win.loss[i])                                                                   #Actual win-loss
}
# rank.robust <- na.exclude(rank.robust)
ggplot(rank.robust, aes(x=rank_ranger, y=rank_gbm, color=rank_actual)) + scale_color_gradient(low="white", high="black") + geom_point() +
labs(title='gbm vs Ranger')+ xlab("Ranger rank [%]") + ylab("gbm rank [%]") +
xlim(c(0, 100)) + ylim(c(0, 100)) + coord_fixed(ratio=1.3)
ggplot(rank.robust, aes(x=((rank_ranger+rank_gbm)/2), y=rank_actual)) + scale_color_gradient(low="white", high="black") + geom_point() +
labs(title='Actual vs Average rank pred.')+ xlab("Average rank [%]") + ylab("Actual rank [%]") +
xlim(c(0, 100)) + ylim(c(0, 100)) + coord_fixed(ratio=1.3)
ggplot(rank.robust, aes(x=((rank_ranger+rank_gbm+rank_glmnet)/3), y=rank_actual)) + scale_color_gradient(low="white", high="black") + geom_point() +
labs(title='Actual vs Average rank pred.')+ xlab("Average rank [%]") + ylab("Actual rank [%]") +
xlim(c(0, 100)) + ylim(c(0, 100)) + coord_fixed(ratio=1.3)
ggplot(my_val,
aes(x=ranger_pred, y=actual.win.loss, color = Price.Model.end)) + geom_point() + scale_color_gradient(low="white", high="black") +
labs(title='Actual.win.loss vs Pred.win.loss')+ xlab("Pred.win.loss") + ylab("Actual.win.loss") +
xlim(c(-30, 50)) + ylim(c(-50, 100)) + coord_fixed(ratio=0.9)
ini.date.model <- as.Date("2014/03/03")   # Initial date the model is prepared
histo.date.model <- as.Date("2016/06/30")   # date the model is compared to historical info (year earlier than end.date.model?)
end.date.model <- as.Date("2017/06/30")   # Final date the model is prepared -- should be end of month
apply.date.model <- as.Date("2017/09/30") # Date the model is designed to predict win/loss performance -- should be end of month
# Prepare table with stock info
table.pred <- prepare.table(stockInfo, end.date.model, ini.date.model, apply.date.model)
# Removing stocks that may have problems
table.pred <- table.pred[table.pred$Price.Model.end > 0.01 & table.pred$Price.Min > 0.01,]
# Adding to table valuations compared to peers
table.pred <- prepare.table.sector(table.pred)
# Adding historical financial status comparison
table.pred <- add.histo.to.table(table.pred, histo.date.model)
save(table.pred, file = "~/Dropbox/Courses/R/StockModel-I/Figures/Table-3m_2017-09-30.Rda")
save(table.model, file = "~/Dropbox/Courses/R/StockModel-I/Figures/Table-3m_2017-06-30.Rda")
# Using created model to make predictions
table.pred$ranger_pred <- predict(model_ranger, table.pred)
table.pred$gbm_pred <- predict(model_gbm, table.pred)
table.pred$glmnet_pred <- predict(model_glmnet, table.pred)
# Creating table with rankings from the different methods
ordered_actual <-      table.pred[order(table.pred$actual.win.loss),]
ordered_ranger <-      table.pred[order(table.pred$ranger_pred),]  #Ranger
ordered_gbm    <-      table.pred[order(table.pred$gbm_pred),]     #GBM
ordered_glmnet <-      table.pred[order(table.pred$glmnet_pred),]  #GLMNET
rank.pred <- data.frame(Stock.SYM = character(0), rank_ranger = numeric(0), rank_gbm = numeric(0), rank_glmnet = numeric(0),
rank_actual = numeric(0), actual.win.loss = numeric(0), stringsAsFactors=FALSE)
for (i in 1:length(ordered_ranger$Stock.SYM)) {
rank.pred[i,] <- list(ordered_ranger$Stock.SYM[i],                                                                         #Name stock
100.*i/length(ordered_ranger$Stock.SYM),                                                             #Rank ranger
100.*match(ordered_ranger$Stock.SYM[i], ordered_gbm$Stock.SYM)/length(ordered_ranger$Stock.SYM),     #Rank gbm
100.*match(ordered_ranger$Stock.SYM[i], ordered_glmnet$Stock.SYM)/length(ordered_ranger$Stock.SYM),  #Rank glmnet
100.*match(ordered_ranger$Stock.SYM[i], ordered_actual$Stock.SYM)/length(ordered_ranger$Stock.SYM),  #Rank actual
ordered_ranger$actual.win.loss[i])                                                                   #Actual win-loss
}
rank.pred[rank.pred$rank_ranger > 93 & rank.pred$rank_gbm > 93 & rank.pred$rank_glmnet > 93, ]
table.pred[table.pred$Stock.SYM == "ACOR",]
table.pred[table.pred$Stock.SYM == "TOWR",]
rank.pred[(rank.pred$rank_ranger + rank.pred$rank_gbm + rank.pred$rank_glmnet)/3 > 98
rank.pred[rank.pred$rank_ranger > 93 & rank.pred$rank_gbm > 93 & rank.pred$rank_glmnet > 93, ]
rank.pred[(rank.pred$rank_ranger + rank.pred$rank_gbm + rank.pred$rank_glmnet)/3 > 98, ]
rank.pred[(rank.pred$rank_ranger + rank.pred$rank_gbm + rank.pred$rank_glmnet)/3 > 95, ]
rank.pred[rank.pred$Stock.SYM == "SALT", ]
rank.pred[rank.pred$Stock.SYM == "SC", ]
ggplot(rank.robust, aes(x=rank_ranger, y=rank_gbm, color=rank_actual)) + scale_color_gradient(low="white", high="black") + geom_point() +
labs(title='gbm vs Ranger')+ xlab("Ranger rank [%]") + ylab("gbm rank [%]") +
xlim(c(0, 100)) + ylim(c(0, 100)) + coord_fixed(ratio=1.3)
ggplot(rank.robust, aes(x=rank_ranger, y=rank_glmnet, color=rank_actual)) + scale_color_gradient(low="white", high="black") + geom_point() +
labs(title='glmnet vs Ranger')+ xlab("Ranger rank [%]") + ylab("glmnet rank [%]") +
xlim(c(0, 100)) + ylim(c(0, 100)) + coord_fixed(ratio=1.3)
ggplot(rank.robust, aes(x=((rank_ranger+rank_gbm)/2), y=rank_actual)) + scale_color_gradient(low="white", high="black") + geom_point() +
labs(title='Actual vs Average rank pred.')+ xlab("Average rank [%]") + ylab("Actual rank [%]") +
xlim(c(0, 100)) + ylim(c(0, 100)) + coord_fixed(ratio=1.3)
table.pred[table.pred$Stock.SYM == "QUAD",]
table.pred[table.pred$Stock.SYM == "INT",]
table.pred[table.pred$Stock.SYM == "NSP",]
table.pred[table.pred$Stock.SYM == "BIDU",]
install.packages(c("assertthat", "backports", "BH", "bmp", "boot", "broom", "car", "caret", "cluster", "curl", "data.table", "DBI", "devtools", "diagram", "doParallel", "dplyr", "drat", "evaluate", "forecast", "foreign", "gbm", "geosphere", "git2r", "glmnet", "h2o", "htmltools", "htmlwidgets", "httr", "hunspell", "imager", "janeaustenr", "jsonlite", "knitr", "lattice", "lazyeval", "lme4", "lubridate", "magick", "mapproj", "maps", "markdown", "MASS", "Matrix", "memoise", "mgcv", "openssl", "pbkrtest", "psych", "purrr", "quantmod", "quantreg", "R6", "ranger", "Rcpp", "RcppArmadillo", "RcppEigen", "rmarkdown", "rpart", "rstudioapi", "scales", "shape", "sp", "SparseM", "stringi", "survival", "tibble", "tidyr", "tidytext", "timeDate", "visNetwork", "withr"))
install.packages(c("assertthat", "backports", "BH", "bmp", "boot", "broom", "car", "caret", "cluster", "curl", "data.table", "DBI", "devtools", "diagram", "doParallel", "dplyr", "drat", "evaluate", "forecast", "foreign", "gbm", "geosphere", "git2r", "glmnet", "h2o", "htmltools", "htmlwidgets", "httr", "hunspell", "imager", "janeaustenr", "jsonlite", "knitr", "lattice", "lazyeval", "lme4", "lubridate", "magick", "mapproj", "maps", "markdown", "MASS", "Matrix", "memoise", "mgcv", "openssl", "pbkrtest", "psych", "purrr", "quantmod", "quantreg", "R6", "ranger", "Rcpp", "RcppArmadillo", "RcppEigen", "rmarkdown", "rpart", "rstudioapi", "scales", "shape", "sp", "SparseM", "stringi", "survival", "tibble", "tidyr", "tidytext", "timeDate", "visNetwork", "withr"))
install.packages(c("assertthat", "backports", "BH", "bmp", "boot", "broom", "car", "caret", "cluster", "curl", "data.table", "DBI", "devtools", "diagram", "doParallel", "dplyr", "drat", "evaluate", "forecast", "foreign", "gbm", "geosphere", "git2r", "glmnet", "h2o", "htmltools", "htmlwidgets", "httr", "hunspell", "imager", "janeaustenr", "jsonlite", "knitr", "lattice", "lazyeval", "lme4", "lubridate", "magick", "mapproj", "maps", "markdown", "MASS", "Matrix", "memoise", "mgcv", "openssl", "pbkrtest", "psych", "purrr", "quantmod", "quantreg", "R6", "ranger", "Rcpp", "RcppArmadillo", "RcppEigen", "rmarkdown", "rpart", "rstudioapi", "scales", "shape", "sp", "SparseM", "stringi", "survival", "tibble", "tidyr", "tidytext", "timeDate", "visNetwork", "withr"))
setwd("~/Dropbox/Courses/R/StockModel-I")
detach("package:datasets", unload=TRUE)
detach("package:graphics", unload=TRUE)
install.packages(c("assertthat", "backports", "BH", "bmp", "boot", "broom", "car", "caret", "cluster", "curl", "data.table", "DBI", "devtools", "diagram", "doParallel", "dplyr", "drat", "evaluate", "forecast", "foreign", "gbm", "geosphere", "git2r", "glmnet", "h2o", "htmltools", "htmlwidgets", "httr", "hunspell", "imager", "janeaustenr", "jsonlite", "knitr", "lattice", "lazyeval", "lme4", "lubridate", "magick", "mapproj", "maps", "markdown", "MASS", "Matrix", "memoise", "mgcv", "openssl", "pbkrtest", "psych", "purrr", "quantmod", "quantreg", "R6", "ranger", "Rcpp", "RcppArmadillo", "RcppEigen", "rmarkdown", "rpart", "rstudioapi", "scales", "shape", "sp", "SparseM", "stringi", "survival", "tibble", "tidyr", "tidytext", "timeDate", "visNetwork", "withr"))
library(googleVis)
load(file = "~/Dropbox/Courses/R/StockModel-I/Figures/imp_par.Rda")
tab1 <- gvisTable(head(imp_par, 13))
print(tab1, "chart")
unlink('ReadMeStockModel_cache', recursive = TRUE)
library(knitr)
knit(input = "ReadMeStockModel.Rmd", output = "README.md")
install.packages("markdown")
install.packages("knitr")
install.packages("knitr")
knit_with_parameters('~/Dropbox/Courses/R/StockModel-I/ReadMeStockModel.Rmd')
knit_with_parameters('~/Dropbox/Courses/R/StockModel-I/ReadMeStockModel.Rmd')
knit(input = "ReadMeStockModel.Rmd", output = "README.md")
library(knitr)
knit(input = "ReadMeStockModel.Rmd", output = "README.md")
